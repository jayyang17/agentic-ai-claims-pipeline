{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoProcessor, Qwen2_5_VLForConditionalGeneration\n",
    "from qwen_vl_utils import process_vision_info\n",
    "from PIL import Image\n",
    "import torch\n",
    "import json\n",
    "import fitz  # PyMuPDF\n",
    "from PIL import Image\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n",
    "    \"Qwen/Qwen2.5-VL-3B-Instruct\",\n",
    "    torch_dtype=torch.float16 if device == \"cuda\" else torch.float32,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "processor = AutoProcessor.from_pretrained(\"Qwen/Qwen2.5-VL-3B-Instruct\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare chat message\n",
    "image_path = r'C:\\Users\\lee_jayyang\\Data_Projects\\RCS\\artifacts\\ingestion\\att_output\\MC 16052025.jpg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"image\", \"image\": image_path},\n",
    "            {\"type\": \"text\", \"text\": (\n",
    "                \"Extract the following fields from this document image:\\n\"\n",
    "                \"- patient_name\\n\"\n",
    "                \"- nric\\n\"\n",
    "                \"- visit_date\\n\"\n",
    "                \"- visit_number\\n\\n\"\n",
    "                \"Return a valid JSON object with exactly these keys in snake_case.\\n\"\n",
    "                \"Do not include explanations, markdown formatting, or text outside the JSON. \"\n",
    "                \"Do not use ```json or ``` wrappers.\\n\"\n",
    "                \"Only respond with raw JSON, like:\\n\"\n",
    "                \"Return blank if not sure, do not create synthetic data\"\n",
    "                \"{\\n\"\n",
    "                \"  \\\"patient_name\\\": \\\"...\\\",\\n\"\n",
    "                \"  \\\"nric\\\": \\\"...\\\",\\n\"\n",
    "                \"  \\\"visit_date\\\": \\\"...\\\",\\n\"\n",
    "                \"  \\\"visit_number\\\": \\\"...\\\"\\n\"\n",
    "                \"}\"\n",
    "            )}\n",
    "        ]\n",
    "    }\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process image + text\n",
    "text = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "image_inputs, video_inputs = process_vision_info(messages)\n",
    "\n",
    "inputs = processor(\n",
    "    text=[text],\n",
    "    images=image_inputs,\n",
    "    videos=video_inputs,\n",
    "    padding=True,\n",
    "    return_tensors=\"pt\"\n",
    ").to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Generate output\n",
    "with torch.no_grad():\n",
    "    generated_ids = model.generate(**inputs, max_new_tokens=512)\n",
    "\n",
    "# Decode only the new generation part\n",
    "generated_ids_trimmed = [\n",
    "    out_ids[len(in_ids):] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n",
    "]\n",
    "output_text = processor.batch_decode(\n",
    "    generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n",
    ")[0]\n",
    "\n",
    "# Try parsing to JSON\n",
    "try:\n",
    "    extracted = json.loads(output_text.strip())\n",
    "except Exception:\n",
    "    extracted = {\"error\": \"Could not parse\", \"raw\": output_text}\n",
    "\n",
    "print(json.dumps(extracted, indent=2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Convert PDF to images\n",
    "def convert_pdf_to_images(pdf_path, output_dir=\"pdf_pages\", dpi=144, resize_width=960):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    doc = fitz.open(pdf_path)\n",
    "    image_paths = []\n",
    "\n",
    "    for i in range(len(doc)):\n",
    "        page = doc.load_page(i)\n",
    "        pix = page.get_pixmap(dpi=dpi)\n",
    "        img_path = os.path.join(output_dir, f\"page_{i+1}.png\")\n",
    "        pix.save(img_path)\n",
    "\n",
    "        # Resize to prevent memory blowup\n",
    "        image = Image.open(img_path)\n",
    "        if image.width > resize_width:\n",
    "            aspect_ratio = image.height / image.width\n",
    "            new_height = int(resize_width * aspect_ratio)\n",
    "            image = image.resize((resize_width, new_height))\n",
    "            image.save(img_path)\n",
    "\n",
    "        image_paths.append(img_path)\n",
    "\n",
    "    return image_paths\n",
    "\n",
    "# Step 2: Extract fields from one image\n",
    "def extract_fields_from_image(image_path, fields):\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": (\n",
    "                \"You are an expert insurance claim processor specializing in reading scanned claim forms \"\n",
    "                \"and extracting structured data for digital processing. You understand insurance-specific \"\n",
    "                \"terminology such as 'ward class', 'MC serial number', 'consultation', and 'ineligible amount'.\"\n",
    "            )\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"image\", \"image\": image_path},\n",
    "                {\"type\": \"text\", \"text\": (\n",
    "                    \"Extract the following fields from this insurance claim document image:\\n\"\n",
    "                    + \"\\n\".join(f\"- {f}\" for f in fields) +\n",
    "                    \"\\n\\nRespond with only a valid JSON object.\\n\"\n",
    "                    \"Do not include any explanation, markdown, or comments. Use snake_case keys exactly as listed above.\\n\"\n",
    "                    \"Leave missing fields as empty strings (\\\"\\\") or null values.\\n\"\n",
    "                    \"Output example:\\n\"\n",
    "                    \"{\\n  \\\"patient_name\\\": \\\"...\\\",\\n  \\\"visit_date\\\": \\\"...\\\",\\n  ...\\n}\"\n",
    "                )}\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    text = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    image_inputs, video_inputs = process_vision_info(messages)\n",
    "\n",
    "    inputs = processor(\n",
    "        text=[text],\n",
    "        images=image_inputs,\n",
    "        videos=video_inputs,\n",
    "        padding=True,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(\"cpu\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        generated_ids = model.generate(**inputs, max_new_tokens=1024)\n",
    "\n",
    "    trimmed_ids = [out[len(inp):] for inp, out in zip(inputs.input_ids, generated_ids)]\n",
    "    output = processor.batch_decode(trimmed_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n",
    "\n",
    "    try:\n",
    "        return json.loads(output.strip())\n",
    "    except Exception:\n",
    "        return {\"error\": \"parse_failed\", \"raw\": output.strip()}\n",
    "\n",
    "# Step 3: Extract from full PDF\n",
    "def extract_from_pdf(pdf_path, field_list):\n",
    "    pages = convert_pdf_to_images(pdf_path)\n",
    "    full_result = {}\n",
    "\n",
    "    for i, image_path in enumerate(pages, start=1):\n",
    "        print(f\"üîç Processing page {i}...\")\n",
    "        result = extract_fields_from_image(image_path, field_list)\n",
    "        full_result[f\"page_{i}\"] = result\n",
    "        os.remove(image_path)\n",
    "\n",
    "    output_path = os.path.splitext(pdf_path)[0] + \"_extracted.json\"\n",
    "    with open(output_path, \"w\") as f:\n",
    "        json.dump(full_result, f, indent=2)\n",
    "\n",
    "    print(f\"\\n‚úÖ Extraction complete. Output saved to:\\n{output_path}\")\n",
    "    return full_result\n",
    "\n",
    "# Define field list\n",
    "fields = [\n",
    "    \"patient_name\", \"patient_nric\", \"clinic_name\", \"service_type\", \"visit_type\", \"visit_date\",\n",
    "    \"mc_from_date\", \"mc_to_date\", \"mc\", \"mc_serial_number\", \"doctor\", \"diagnosis\",\n",
    "    \"invoice_no\", \"invoice_date\"\n",
    "]\n",
    "\n",
    "# Run POC\n",
    "extract_from_pdf(\n",
    "    r\"C:\\Users\\lee_jayyang\\Data_Projects\\RCS\\artifacts\\ingestion\\att_output\\Group Outpatient Medical Claim Form (1 May 2025) - Yenny.pdf\",\n",
    "    fields\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_claim_fields(pages_dict):\n",
    "    merged = {}\n",
    "\n",
    "    for page_key in sorted(pages_dict.keys()):\n",
    "        page = pages_dict[page_key]\n",
    "\n",
    "        # If parse failed, try to recover the raw JSON\n",
    "        if \"raw\" in page:\n",
    "            try:\n",
    "                raw = page[\"raw\"]\n",
    "                raw = raw.strip().removeprefix(\"```json\").removesuffix(\"```\").strip()\n",
    "                page = json.loads(raw)\n",
    "            except:\n",
    "                continue  # skip page if even raw can't be parsed\n",
    "\n",
    "        for k, v in page.items():\n",
    "            if not merged.get(k) and v not in [None, \"\", \"null\"]:\n",
    "                merged[k] = v  # take first non-empty value\n",
    "\n",
    "    # Fill in empty strings for any missing fields\n",
    "    all_fields = set().union(*[d.keys() for d in pages_dict.values() if isinstance(d, dict)])\n",
    "    for field in all_fields:\n",
    "        merged.setdefault(field, \"\")\n",
    "\n",
    "    return merged\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_json_path = r'C:\\Users\\lee_jayyang\\Data_Projects\\RCS\\artifacts\\ingestion\\att_output\\Group Outpatient Medical Claim Form (1 May 2025) - Yenny_extracted.json'\n",
    "with open(input_json_path, 'r', encoding='utf-8') as file:\n",
    "    json_list = json.load(file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Merge into one flat claim\n",
    "merged_claim = merge_claim_fields(json_list)\n",
    "\n",
    "\n",
    "print(\"üßæ Final merged claim:\")\n",
    "print(json.dumps(merged_claim, indent=2))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "crewai_qwen",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
